@startuml
title Class Diagram
skinparam titleFontSize 36 

package Models {
    interface ModelInterface {
        + process(input: Any): Any
        + load_weights(weights: List[Tuple[str, mx.array]]): None
        + get_input_embeddings(input_ids: Optional[mx.array] = None, pixel_values: Optional[mx.array] = None): mx.array
    }

    abstract class BaseModel {
        - vision: VisionInterface
        - language: LanguageInterface
        
        - generate_step(logits: mx.array, cache: Any, temperature: float): Tuple[int, Any]
        - sample(logits: mx.array, temperature: float): int
        - get_model_path(): str
        - load_model(path_or_hf_repo: str): model, processor
        + process(input: Any): Any
        + load_weights(weights: List[Tuple[str, mx.array]]): None
        + get_input_embeddings(input_ids: Optional[mx.array] = None, pixel_values: Optional[mx.array] = None): mx.array
        + load_model(path_or_hf_repo: str): None
        # {abstract} __call__(input_ids: mx.array, pixel_values: mx.array, mask: Optional[mx.array] = None, cache=None): Tuple[mx.array, Any]
        # {abstract} from_pretrained(path_or_hf_repo: str): BaseModel
        
    }

    abstract class VisionBase {
    - config: VisionConfig
    - patch_embedding: nn.Conv2d
    - position_embedding: nn.Embedding
    - class_embedding: mx.array
    - encoder_layers: List[Dict]
    - post_layernorm: nn.LayerNorm

    + __init__(config: VisionConfig)
    + __call__(x: mx.array, output_hidden_states: Optional[bool] = None): mx.array
    - _create_encoder_layer(): Dict
    - _create_attention(): Dict
    - _create_mlp(): Dict
    - _embed(x: mx.array): mx.array
    - _encode(x: mx.array, output_hidden_states: Optional[bool] = None): Tuple[mx.array, Optional[Tuple[mx.array, ...]]]
    - _encoder_layer_forward(x: mx.array, layer: Dict): mx.array
    - _attention_forward(x: mx.array, attn: Dict): mx.array
    - _mlp_forward(x: mx.array, mlp: Dict): mx.array
    + sanitize(weights: Dict): Dict
    - {static} _check_array_shape(arr: mx.array): bool
    + {static} from_dict(params: Dict): Vision
    }

    class VisionConfig {
        + model_type: str
        + num_hidden_layers: int
        + hidden_size: int
        + intermediate_size: int
        + num_attention_heads: int
        + image_size: int
        + patch_size: int
        + layer_norm_eps: float
        + num_channels: int = 3
        + projection_dim: Optional[int] = None
        + vocab_size: Optional[int] = None

        + {static} from_dict(params: Dict): VisionConfig
    }

    VisionBase --> VisionConfig: HAS A

    abstract class LanguageBase {
        - config: LanguageConfig
        - embed_tokens: nn.Embedding
        - position_embedding: nn.Embedding
        - layers: List[Dict]
        - norm: nn.RMSNorm
        - lm_head: nn.Linear

        + __init__(config: LanguageConfig)
        # {abstract} __call__(input_ids: mx.array, attention_mask: Optional[mx.array] = None, position_ids: Optional[mx.array] = None, cache: Optional[Tuple[mx.array, mx.array]] = None): Tuple[mx.array, Optional[Tuple[mx.array, mx.array]]]
        - _create_transformer_block(): Dict
        - _create_attention(): Dict
        - _create_mlp(): Dict
        - _embed(input_ids: mx.array, position_ids: Optional[mx.array] = None): mx.array
        - _encode(x: mx.array, attention_mask: Optional[mx.array], cache: Optional[Tuple[mx.array, mx.array]]): Tuple[mx.array, Optional[Tuple[mx.array, mx.array]]]
        - _transformer_block_forward(x: mx.array, layer: Dict, attention_mask: Optional[mx.array], cache: Optional[Tuple[mx.array, mx.array]]): Tuple[mx.array, Optional[Tuple[mx.array, mx.array]]]
        - _attention_forward(x: mx.array, attn: Dict, mask: Optional[mx.array] = None, cache: Optional[Tuple[mx.array, mx.array]] = None): Tuple[mx.array, Tuple[mx.array, mx.array]]
        - _mlp_forward(x: mx.array, mlp: Dict): mx.array
        + sanitize(weights: Dict): Dict
        + {static} from_dict(params: Dict): LanguageBase
    }

    class LanguageConfig {
        + model_type: str
        + hidden_size: int
        + num_hidden_layers: int
        + intermediate_size: int
        + num_attention_heads: int
        + max_position_embeddings: int
        + vocab_size: int
        + num_key_value_heads: Optional[int]
        + rms_norm_eps: float
        + rope_theta: float
        + rope_traditional: bool
        + layer_norm_eps: float
        + pad_token_id: int

        + {static} from_dict(params: Dict): LanguageConfig
    }

    LanguageBase --> LanguageConfig: HAS A
    BaseModel --> LanguageBase: HAS A

    class Idefics2 {
        + __call__(input_ids: mx.array, pixel_values: mx.array, mask: Optional[mx.array] = None, cache=None): Tuple[mx.array, Any]
    }

    class Llava {
        + __call__(input_ids: mx.array, pixel_values: mx.array, mask: Optional[mx.array] = None, cache=None): Tuple[mx.array, Any] 
    }

    class NanoLlava {
        + __call__(input_ids: mx.array, pixel_values: mx.array, mask: Optional[mx.array] = None, cache=None): Tuple[mx.array, Any]
    }

    ModelInterface <|.. BaseModel: IMPLEMENTS
    BaseModel <|-- Idefics2: IS A
    BaseModel <|-- Llava: IS A 
    BaseModel <|-- NanoLlava: IS A
    
}

package MediaEngine {
    interface MediaEngineInterface {
        + process(input: Any): Any
    }
    note right of MediaEngineInterface
    in the next step, in order to add real time processing, 
    we can use the observer pattern to accept input of video stream
    and when the frame is changed we can process the frame to
    explain what we're seeing
    end note

    abstract class BaseImageProcessor {
        # image_mean: Tuple[float, float, float]
        # image_std: Tuple[float, float, float] 
        # size: Tuple[int, int]
        # crop_size: Dict[str, int]
        # resample: PILImageResampling
        # rescale_factor: float
        # data_format: ChannelDimension

        - load_engine(model_path: str, engine_type: str): None
        + {abstract} preprocess(images: Any): Any
    }

    class SingleImageMediaEngine {
        + process(input: Any): Any
    }

    class BatchImageMediaEngine {
        + process(input: Any): Any
    }

    class VideoMediaEngine {
        + process(input: Any): Any
    }
    
    BaseImageProcessor <|-- SingleImageMediaEngine: IS A
    BaseImageProcessor <|-- BatchImageMediaEngine: IS A
    BaseImageProcessor <|-- VideoMediaEngine: IS A
    MediaEngineInterface <-- BaseImageProcessor: IMPLEMENTS

}

package EntryPoint {
    abstract class EntryPointInterface {
        + {abstract} initMediaEngine(): MediaEngineInterface
        + {abstract} initModel(): ModelInterface
        + execute()
    }
    
    class ChatUI {
        + initMediaEngine(): MediaEngineInterface
        + initModel(): ModelInterface
        + execute()
    }

    class CLI {
        + initMediaEngine(): MediaEngineInterface
        + initModel(): ModelInterface
        + execute()
    }

    class Generate {
        + initMediaEngine(): MediaEngineInterface
        + initModel(): ModelInterface
        + execute()
    }

    EntryPointInterface <|-- ChatUI: IS A
    EntryPointInterface <|-- CLI: IS A
    EntryPointInterface <|-- Generate: IS A

    EntryPointInterface --> MediaEngineInterface: CREATES
    EntryPointInterface --> ModelInterface: CREATES
    
}
package core {
    class Utils {
        + parse_arguments(): parsr.parse_args()
        + load_image(image_path: str): Any
    }

    class Tokenizer {
        + encode(text: str): List[int]
        + decode(tokens: List[int]): str
    }

    class ConfigManager {
        - load_config(model_path: str): None
        + get_config(): dict
    }

    class Logger {
        info(message: str): None
        warn(message: str): None
        error(message: str): None
        debug(message: str): None
    }

    class ErrorHandler {
        handle_exception(e: Exception, message: str): None
        handle_not_implemented_error(message: str): None
    }

}

@enduml